{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER ANALYTICS \n",
    "## Text Classication & Sentiment Analysis\n",
    "### Musarath J. Rahamathullah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Libraries](#lib_header)\n",
    "* [Data Collection](#Data-bullet)\n",
    "* [Data Preprocessing](#DP-bullet)\n",
    "* [Word Cloud](#wc-bullet)\n",
    "* [Latent Dirichlet Allocation](#lda-bullet)\n",
    "* [Text Classification Models](#cm-bullet)\n",
    "    * [Navie Bayes](#nb-bullet)\n",
    "    * [Linear Classifier](#lc-bullet)\n",
    "    * [Support Vector Machine](#svm-bullet)\n",
    "    * [Random Forest](#rf-bullet)\n",
    "    * [Extreme Gradient Boosting](#xg-bullet)\n",
    "    * [Convoluntional Neural Network](#cnn-bullet)\n",
    "* [Classifier Interface](#ci-bullet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES <a class=\"anchor\" id=\"lib_header\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#load libraries\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import wget\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import webbrowser\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import xgboost, string\n",
    "\n",
    "import tensorflow\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "from datetime import datetime\n",
    "from plotly import graph_objs as go\n",
    "import plotly.tools as plotly_tools\n",
    "from plotly.offline import init_notebook_mode,plot\n",
    "from plotly import tools\n",
    "\n",
    "import chart_studio \n",
    "chart_studio.tools.set_credentials_file(username='musarath', api_key='hN1B3D9TpmpUSxRG5iKy')\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from flask import Flask, request, render_template, jsonify\n",
    "import import_ipynb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION<a class=\"anchor\" id=\"Data-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets from twitter \n",
    "def twitter_credentials(hashtag):\n",
    "    # Twitter API credentials\n",
    "    with open('twitter_credentials.json') as cred_data:\n",
    "        info = json.load(cred_data)\n",
    "        consumer_key = info['CONSUMER_KEY']\n",
    "        consumer_secret = info['CONSUMER_SECRET']\n",
    "        access_key = info['ACCESS_KEY']\n",
    "        access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "    # Create the api endpoint\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "    twts = []\n",
    "    print(\"Downloading in progress...\")\n",
    "    for tweet in tweepy.Cursor( api.search, q='#' + hashtag  + \"-filter:retweets\",\n",
    "                                #api.search, q='#' + hashtag + since:2019-09-01 + until:2019-9-22 +  \"-filter:retweets\" ,\n",
    "                               lang=\"en\").items(10000):\n",
    "        twts.append([str(tweet.created_at),tweet.text.encode('utf-8')])\n",
    "    print(\"Downloading completed!\")\n",
    "\n",
    "     \n",
    "    return twts\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Uncomment these code to downlaod tweets ###\n",
    "# # Mention the maximum number of tweets that you want to be extracted.\n",
    "# #maximum_number_of_tweets_to_be_extracted = int(input('Enter the number of tweets that you want to extract- '))\n",
    "\n",
    "# # Mention the hashtag that you want to look out for\n",
    "# hashtag = input('Enter the hashtag you want to scrape- ')\n",
    "# # tweets download\n",
    "# twts_download = twitter_credentials(hashtag)\n",
    "\n",
    "# print(\"Length of tweets:\",len(twts_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "def write_tweets_to_file(hash,tweets):\n",
    "    with open('tweets_with_hashtag_'+hash+'.csv', 'w',encoding='utf8',newline='') as the_file:\n",
    "        writer = csv.writer(the_file)\n",
    "        writer.writerow(['created_at', 'text'])\n",
    "        writer.writerows(tweets)\n",
    "        \n",
    "# call the function        \n",
    "#write_tweets_to_file(hashtag,twts_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross check the tweets saved\n",
    "def read_tweets_from_file(hash):\n",
    "    tweets = []\n",
    "    with open('tweets_with_hashtag_'+hash+'.csv', 'r') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:\n",
    "            if \"fruit\" not in row:\n",
    "                tweets.append(row)\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# # call read from file function\n",
    "#tweets_data = read_tweets_from_file(hashtag)\n",
    "#print ('Extracted ' + str(len(tweets_data)) + ' tweets with hashtag #' + hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tweets = pd.DataFrame(tweets_data)\n",
    "# get_tweets.rename(columns=get_tweets.iloc[0],inplace = True)\n",
    "# get_tweets.drop(get_tweets.index[0],inplace = True)\n",
    "# print(get_tweets['text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING<a class=\"anchor\" id=\"DP-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove http link in tweet text\n",
    "def remove_http(tweets):\n",
    "    http_removed_tweets = []\n",
    "    for idx,row in enumerate(tweets):\n",
    "        http_removed_tweets.append(re.sub('[n]?http[s]?://\\S+', '', row))\n",
    "    return http_removed_tweets   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets text cleanup\n",
    "#to remove the pattern ‘@user’ from all the tweets in our data.\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt \n",
    "\n",
    "def tweets_clean_up(get_tweets):\n",
    "\n",
    "    get_tweets = pd.DataFrame(get_tweets)\n",
    "    get_tweets.rename(columns=get_tweets.iloc[0],inplace = True)\n",
    "    get_tweets.drop(get_tweets.index[0],inplace = True)\n",
    "    #print(get_tweets.head())\n",
    "\n",
    "    # remove http link\n",
    "    get_tweets['tidy_tweet'] = remove_http(get_tweets['text'])\n",
    "\n",
    "    # remove twitter handles (@user)\n",
    "    get_tweets['tidy_tweet'] = np.vectorize(remove_pattern)(get_tweets['tidy_tweet'], \"@[\\w]*\")\n",
    "    \n",
    "    # remove special characters, numbers, punctuations\n",
    "    get_tweets['tidy_tweet'] = get_tweets['tidy_tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "        \n",
    "    # removing the short words\n",
    "    get_tweets['tidy_tweet'] = get_tweets['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "        \n",
    "    return get_tweets\n",
    "    \n",
    "# # tweets cleanup function\n",
    "#tweets_data = tweets_clean_up(tweets_data)\n",
    "\n",
    "# # write to csv\n",
    "# tweets_data.to_csv('tweets_with_cleaned_hashtag_'+hashtag+'.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization method\n",
    "def tweet_tokenization(tweet):\n",
    "    # Tokenization\n",
    "    tokenized_tweet = tweet['tidy_tweet'].apply(lambda x: x.split())    \n",
    "    return tokenized_tweet\n",
    "\n",
    "# # tokenize the tweets\n",
    "#tweets_data['tokens'] =tweet_tokenization(tweets_data)\n",
    "#tweets_data['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming / Lemmatizier\n",
    "def tweet_reduce_wordforms(tokenized_tweet):\n",
    "    #Stemming\n",
    "    #tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "    #tokenized_tweet.head()\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [lmtzr.lemmatize(i,'v') for i in x])\n",
    "    \n",
    "    for i in range(1,len(tokenized_tweet)+1):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    return tokenized_tweet\n",
    "\n",
    "# # get tokens to baseform\n",
    "#tweets_data['tokens'] = tweet_reduce_wordforms(tweets_data['tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD CLOUD<a class=\"anchor\" id=\"wc-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word cloud method\n",
    "# def tweets_word_cloud(tweets):\n",
    "#     # worldcloud of tweets\n",
    "#     all_words = ' '.join([text for text in tweets['tokens']])\n",
    "\n",
    "#     #import worldCloud\n",
    "#     wordcloud = WordCloud(width=800, height=500, random_state=21,background_color= 'white',max_words=100, max_font_size=110).generate(all_words)\n",
    "\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()tweets_daily_count = df_tweets.groupby('created_at')['tidy_tweet'].count()\n",
    "# tweets_daily_count.plot(kind='bar', x='Date')\n",
    "    \n",
    "# #tweets_word_cloud(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tweets_data\n",
    "\n",
    "# tmp['Date'] = tmp.created_at.apply(lambda x: pd.Series(str(x).split(\" \")[0]))\n",
    "# #print(d)\n",
    "# tweets_daily_count = tmp.groupby('Date')['tidy_tweet'].count()\n",
    "# pt = tweets_daily_count.plot(kind='bar', x='Date')\n",
    "# # Set the y-axis label\n",
    "# pt.set_ylabel(\"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LATENT DIRICHLET ALLOCATION<a class=\"anchor\" id=\"lda-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read all the titles\n",
    "# titles = []\n",
    "# for tweet in tweets_data['tokens']:\n",
    " \n",
    "#     titles.append(tweet) \n",
    "    \n",
    "# print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a count vectorizer object \n",
    "# count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "# count_vect.fit(titles)\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "# xtrain_count =  count_vect.transform(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train a LDA Model\n",
    "# def lda_mode(title):\n",
    "\n",
    "#     lda_model = decomposition.LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=20)\n",
    "#     X_topics = lda_model.fit_transform(xtrain_count)\n",
    "#     topic_word = lda_model.components_ \n",
    "#     vocab = count_vect.get_feature_names()\n",
    "\n",
    "#     # view the topic models\n",
    "#     n_top_words = 10\n",
    "#     topic_summaries = []\n",
    "#     for i, topic_dist in enumerate(topic_word):\n",
    "#         topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "#         topic_summaries.append(' '.join(topic_words))\n",
    "        \n",
    "#     return topic_summaries\n",
    "\n",
    "# topic_summary = lda_mode(titles)\n",
    "# topic_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT CLASSIFICATION MODELS<a class=\"anchor\" id=\"cm-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_data = pd.read_csv(\"tweets_with_cleaned_hashtag_samsung_classification.csv\")\n",
    "train_DF = pd.DataFrame()\n",
    "train_DF['tweet_text'] = model_train_data['tidy_tweet']\n",
    "train_DF['label'] = model_train_data['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_DF['tweet_text'], train_DF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_word = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_word.fit(train_DF['tweet_text'])\n",
    "\n",
    "xtrain_tfidf_word =  tfidf_word.transform(train_x)\n",
    "xvalid_tfidf_word =  tfidf_word.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_ngram.fit(train_DF['tweet_text'])\n",
    "xtrain_tfidf_ngram =  tfidf_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_chars.fit(train_DF['tweet_text'])\n",
    "xtrain_tfidf_chars =  tfidf_chars.transform(train_x) \n",
    "xvalid_tfidf_chars =  tfidf_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_DF['char_count'] = train_DF['tweet_text'].apply(len)\n",
    "# train_DF['word_count'] = train_DF['tweet_text'].apply(lambda x: len(x.split()))\n",
    "# train_DF['word_density'] = train_DF['char_count'] / (train_DF['word_count']+1)\n",
    "# train_DF['punctuation_count'] = train_DF['tweet_text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "# train_DF['title_word_count'] = train_DF['tweet_text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# train_DF['upper_case_word_count'] = train_DF['tweet_text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.classification_report(predictions, valid_y)\n",
    "#accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES<a class=\"anchor\" id=\"nb-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, WordLevel TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.96      0.75        28\n",
      "           1       0.42      0.64      0.50        44\n",
      "           2       0.96      0.47      0.63       154\n",
      "           3       0.29      1.00      0.45        13\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.59       239\n",
      "   macro avg       0.46      0.61      0.47       239\n",
      "weighted avg       0.78      0.59      0.61       239\n",
      "\n",
      "Naive Bayes, N-Gram Vectors:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        20\n",
      "           1       0.36      0.63      0.46        38\n",
      "           2       0.88      0.39      0.54       172\n",
      "           3       0.20      1.00      0.33         9\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.50       239\n",
      "   macro avg       0.38      0.60      0.39       239\n",
      "weighted avg       0.74      0.50      0.53       239\n",
      "\n",
      "Naive Bayes, CharLevel TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.85      0.73        33\n",
      "           1       0.40      0.75      0.52        36\n",
      "           2       0.95      0.44      0.60       163\n",
      "           3       0.16      1.00      0.27         7\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.56       239\n",
      "   macro avg       0.43      0.61      0.42       239\n",
      "weighted avg       0.80      0.56      0.60       239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(naive_bayes.MultinomialNB(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"Naive Bayes, WordLevel TF-IDF:\\n\" , accuracy)\n",
    "#print (\"Naive Bayes, WordLevel TF-IDF: {}%\".format(round(accuracy*100,2)))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"Naive Bayes, N-Gram Vectors:\\n\" , accuracy)\n",
    "#print( \"Navie Bayes, N-Gram Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(naive_bayes.MultinomialNB(), xtrain_tfidf_chars, train_y, xvalid_tfidf_chars)\n",
    "print (\"Naive Bayes, CharLevel TF-IDF:\\n\" , accuracy)\n",
    "#print (\"Navie Bayes, CharLevel Vectors: {}%\".format(round(accuracy*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR CLASSIFIER<a class=\"anchor\" id=\"lc-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning:\n",
      "\n",
      "Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning:\n",
      "\n",
      "Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, WordLevel TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.93      0.70        27\n",
      "           1       0.73      0.67      0.70        73\n",
      "           2       0.95      0.63      0.76       114\n",
      "           3       0.51      0.92      0.66        25\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.71       239\n",
      "   macro avg       0.55      0.63      0.56       239\n",
      "weighted avg       0.79      0.71      0.72       239\n",
      "\n",
      "Logistic Regression, N-Gram Vectors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      1.00      0.51        15\n",
      "           1       0.33      0.76      0.46        29\n",
      "           2       0.92      0.37      0.53       187\n",
      "           3       0.18      1.00      0.30         8\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.48       239\n",
      "   macro avg       0.35      0.63      0.36       239\n",
      "weighted avg       0.79      0.48      0.51       239\n",
      "\n",
      "Logistic Regression, CharLevel Vectors:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91        42\n",
      "           1       0.87      0.74      0.80        78\n",
      "           2       0.93      0.81      0.87        88\n",
      "           3       0.67      1.00      0.80        30\n",
      "           4       0.14      1.00      0.25         1\n",
      "\n",
      "    accuracy                           0.83       239\n",
      "   macro avg       0.70      0.90      0.72       239\n",
      "weighted avg       0.87      0.83      0.84       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(linear_model.LogisticRegression(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "#print( \"Logistic Regression, WordLevel TF-IDF: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"Logistic Regression, WordLevel TF-IDF:\\n\",accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"Logistic Regression, N-Gram Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print (\"Logistic Regression, N-Gram Vectors: \\n\",accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(linear_model.LogisticRegression(), xtrain_tfidf_chars, train_y, xvalid_tfidf_chars)\n",
    "#print( \"Logistic Regression, CharLevel Vectors:{}%\".format(round(accuracy*100,2)))\n",
    "print( \"Logistic Regression, CharLevel Vectors:\\n\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE<a class=\"anchor\" id=\"svm-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:/n%               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.32      0.48       239\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.32       239\n",
      "   macro avg       0.20      0.06      0.10       239\n",
      "weighted avg       1.00      0.32      0.48       239\n",
      "\n",
      "SVM, N-Gram Vectors:\n",
      "%               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.32      0.48       239\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.32       239\n",
      "   macro avg       0.20      0.06      0.10       239\n",
      "weighted avg       1.00      0.32      0.48       239\n",
      "\n",
      "SVM, CharLevel Vectors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.32      0.48       239\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.32       239\n",
      "   macro avg       0.20      0.06      0.10       239\n",
      "weighted avg       1.00      0.32      0.48       239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(svm.SVC(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "#print( \"SVM, WordLevel TF-IDF: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"SVM, WordLevel TF-IDF:/n%\",accuracy)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"SVM, N-Gram Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print (\"SVM, N-Gram Vectors:\\n%\",accuracy)\n",
    "\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(svm.SVC(), xtrain_tfidf_chars, train_y, xvalid_tfidf_chars)\n",
    "#print( \"SVM, CharLevel Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"SVM, CharLevel Vectors: \\n\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST<a class=\"anchor\" id=\"rf-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, WordLevel TF-IDF: \n",
      "%               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.84      0.78        38\n",
      "           1       0.91      0.62      0.74        98\n",
      "           2       0.80      0.87      0.84        70\n",
      "           3       0.58      0.87      0.69        30\n",
      "           4       0.29      0.67      0.40         3\n",
      "\n",
      "    accuracy                           0.76       239\n",
      "   macro avg       0.66      0.77      0.69       239\n",
      "weighted avg       0.80      0.76      0.76       239\n",
      "\n",
      "Random Forest, N-Gram Vectors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60        19\n",
      "           1       0.81      0.36      0.50       148\n",
      "           2       0.51      0.65      0.57        60\n",
      "           3       0.22      0.83      0.35        12\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.51       239\n",
      "   macro avg       0.39      0.57      0.41       239\n",
      "weighted avg       0.67      0.51      0.52       239\n",
      "\n",
      "Random Forest, CharLevel Vectors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84        42\n",
      "           1       0.87      0.69      0.77        84\n",
      "           2       0.86      0.77      0.81        84\n",
      "           3       0.58      0.93      0.71        28\n",
      "           4       0.14      1.00      0.25         1\n",
      "\n",
      "    accuracy                           0.78       239\n",
      "   macro avg       0.65      0.85      0.68       239\n",
      "weighted avg       0.82      0.78      0.79       239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning:\n",
      "\n",
      "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "\n",
      "C:\\Users\\Musarath\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(ensemble.RandomForestClassifier(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "#print (\"Random Forest, WordLevel TF-IDF: {}%\".format(round(accuracy*100,2)))\n",
    "print (\"Random Forest, WordLevel TF-IDF: \\n%\",accuracy)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"Random Forest, N-Gram Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print (\"Random Forest, N-Gram Vectors: \\n\",accuracy)\n",
    "\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(ensemble.RandomForestClassifier(), xtrain_tfidf_chars,train_y,xvalid_tfidf_chars)\n",
    "#print( \"Random Forest, CharLevel Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"Random Forest, CharLevel Vectors: \\n\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTREME GRADIENT BOOSTING<a class=\"anchor\" id=\"xg-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extereme Gradient Boosting, WordLevel TF-IDF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.77        37\n",
      "           1       0.97      0.65      0.78       100\n",
      "           2       0.79      1.00      0.88        60\n",
      "           3       0.80      0.92      0.86        39\n",
      "           4       0.29      0.67      0.40         3\n",
      "\n",
      "    accuracy                           0.81       239\n",
      "   macro avg       0.71      0.82      0.74       239\n",
      "weighted avg       0.85      0.81      0.81       239\n",
      "\n",
      "Extereme Gradient Boosting, N-Gram Vectors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60        19\n",
      "           1       0.36      0.71      0.48        34\n",
      "           2       0.89      0.40      0.56       168\n",
      "           3       0.24      0.69      0.36        16\n",
      "           4       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.51       239\n",
      "   macro avg       0.39      0.56      0.40       239\n",
      "weighted avg       0.73      0.51      0.53       239\n",
      "\n",
      "Extereme Gradient Boosting, CharLevel Vectors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        41\n",
      "           1       0.97      0.89      0.93        73\n",
      "           2       0.97      0.97      0.97        76\n",
      "           3       0.91      0.93      0.92        44\n",
      "           4       0.57      0.80      0.67         5\n",
      "\n",
      "    accuracy                           0.94       239\n",
      "   macro avg       0.87      0.92      0.89       239\n",
      "weighted avg       0.95      0.94      0.94       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(xgboost.XGBClassifier(), xtrain_tfidf_word.tocsc(), train_y, xvalid_tfidf_word.tocsc())\n",
    "#print( \"Extereme Gradient Boosting, WordLevel TF-IDF: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"Extereme Gradient Boosting, WordLevel TF-IDF: \\n\",accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "#print( \"Extereme Gradient Boosting, N-Gram Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"Extereme Gradient Boosting, N-Gram Vectors: \\n\",accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_classifier_model(xgboost.XGBClassifier(), xtrain_tfidf_chars.tocsc(), train_y, xvalid_tfidf_chars.tocsc())\n",
    "#print( \"Extereme Gradient Boosting, CharLevel Vectors: {}%\".format(round(accuracy*100,2)))\n",
    "print( \"Extereme Gradient Boosting, CharLevel Vectors: \\n\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_final_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "        \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_new_tweets(file):\n",
    "    tweets = []\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:\n",
    "            tweets.append(row)\n",
    "    \n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER INTERFACE<a class=\"anchor\" id=\"ci-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:5051/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [30/Nov/2019 17:18:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musarath\\Musarath\\Anly699_Project\\example.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Nov/2019 17:18:41] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#import HTML\n",
    "import flask_ngrok\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "#run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "# to get information from my-form page\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template('my-form.html')\n",
    "\n",
    "# to post the result to web page\n",
    "@app.route('/',methods=['POST'])\n",
    "def my_analysis():\n",
    "   \n",
    "    # read the batch file\n",
    "    file = request.form['fileupload']    # get the url and store in varaible\n",
    "    print(file)\n",
    "    # read tweets from file\n",
    "    tweets = read_new_tweets(file)\n",
    "    # clean up the tweets\n",
    "    tweets = tweets_clean_up(tweets)\n",
    "    # tokenize\n",
    "    tokens_tfidf =  tfidf_word.transform(tweets['tidy_tweet'])\n",
    "    # predict the tags using classifier model    \n",
    "    pred = train_classifier_final_model(xgboost.XGBClassifier(), xtrain_tfidf_word.tocsc(), train_y, tokens_tfidf.tocsc())\n",
    "    pred = encoder.inverse_transform(pred)\n",
    "    # convert to dataframe\n",
    "    output = pd.DataFrame({\"Tweet\":tweets['tidy_tweet'],\"Tag\":pred})\n",
    "    #print(output)\n",
    "       \n",
    "    \n",
    "    return render_template('my-form.html',result = output.to_html())\n",
    "\n",
    "# main function\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost',port=5051)\n",
    "    #app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code is for Twitter Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_tweets = []\n",
    "# # download data\n",
    "# def twitter_data_download(hash_tag):    \n",
    "    \n",
    "#     # tweets download\n",
    "#     twts_download = twitter_credentials(hash_tag)\n",
    "\n",
    "#     # call the function        \n",
    "#     write_tweets_to_file(hash_tag,twts_download)\n",
    "\n",
    "#     # call read from file function\n",
    "#     twitter_tweets = read_tweets_from_file(hash_tag)\n",
    "#     #print(twitter_tweets)\n",
    "    \n",
    "#     # tweets cleanup function\n",
    "#     twitter_tweets = tweets_clean_up(twitter_tweets)\n",
    "#     #print(twitter_tweets)\n",
    "    \n",
    "#     return twitter_tweets\n",
    "\n",
    "# # call twitter data downlaod\n",
    "# #hashtag = \"samsung\"\n",
    "# #df_tweets = twitter_data_download(hashtag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets_data = []\n",
    "# # function to tag tweets\n",
    "# def twitter_abstract(tweets_data,xtrain_tfidf_word,train_y):\n",
    "#     #tokenize\n",
    "#     tokens_tfidf_word =  tfidf_word.transform(tweets_data['tidy_tweet'])\n",
    "\n",
    "#     pred = train_classifier_final_model(xgboost.XGBClassifier(), xtrain_tfidf_word.tocsc(), train_y, tokens_tfidf_word.tocsc())\n",
    "#     tweets_data['Abstract_class'] = pred\n",
    "#     tweets_data['Abstract'] = encoder.inverse_transform(pred)\n",
    "    \n",
    "#     #convert the date field to date format \n",
    "#     df_tweets_data = pd.DataFrame(tweets_data) \n",
    "\n",
    "#     #convert to date type\n",
    "#     df_tweets_data['created_at'] = pd.to_datetime(df_tweets_data[\"created_at\"])\n",
    "#     df_tweets_data['created_at'] = df_tweets_data['created_at'].dt.date \n",
    "#     # drop the columns\n",
    "#     df_tweets_data.drop(['text','Abstract_class'],axis = 1,inplace = True)\n",
    "#     return df_tweets_data\n",
    "\n",
    "# # call twitter classifier\n",
    "# #df_tweets = twitter_abstract(df_tweets,xtrain_tfidf_word,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method to calculate sentiment of each tweet\n",
    "# def tweets_sentiment(df_tweet):\n",
    "#     # next, we initialize VADER so we can use it within our Python script\n",
    "#     sid = SentimentIntensityAnalyzer()\n",
    "#     sentiment_score = []\n",
    "\n",
    "#     ## Calling the polarity_scores method on sid and passing in the title \n",
    "#     #outputs a dictionary with negative, neutral, positive, and compound scores for the input title\n",
    "#     for tweet in df_tweet['tidy_tweet']:\n",
    "#         scores = sid.polarity_scores(tweet)\n",
    "#         sentiment_score.append(scores['compound'])\n",
    "\n",
    "#     df_tweet['sentiment'] = sentiment_score\n",
    "#     return df_tweet\n",
    "\n",
    "# # sentiment\n",
    "# #df_tweets = tweets_sentiment(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #c onvert datatime to date\n",
    "# df_tweets['Date'] = df_tweets.created_at.apply(lambda x: pd.Series(str(x).split(\" \")[0]))\n",
    "\n",
    "# df_average_sentiment = df_tweets.groupby('Date')['sentiment'].mean()\n",
    "# pt = df_average_sentiment.plot(kind='bar', x='Date',colormap = 'Paired')\n",
    "# # Set the y-axis label\n",
    "# pt.set_ylabel(\"Sentiment_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tweets_data = tweets_polarity(tweets_data)\n",
    "# tweets_data.index.name = 'Id'\n",
    "# #tweets_data.reset_index(drop=True, inplace=True)\n",
    "# tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to define polarity\n",
    "# def tweets_polarity(df_tweets):\n",
    "#     polarity = []\n",
    "#     for t in df_tweets['sentiment']:\n",
    "#         if t<0:\n",
    "#             polarity.append(\"Negative\")\n",
    "#         elif t == 0:\n",
    "#             polarity.append(\"Neutral\")\n",
    "#         else:\n",
    "#             polarity.append(\"Positive\")\n",
    "#     df_tweets['Polarity'] = polarity\n",
    "#     return df_tweets\n",
    "\n",
    "# # call polarity\n",
    "# #df_tweets = tweets_polarity(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_elements, counts_elements = np.unique(df_tweets['Abstract'], return_counts=True)\n",
    "# counts_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function for number of tweets per abstract over time\n",
    "# def tweets_over_time(df_tweets):\n",
    "#     df_daily_count = df_tweets.groupby(['created_at','Abstract'],as_index = False).agg({'tidy_tweet': \"count\"})\n",
    "\n",
    "#     tmp = df_daily_count[df_daily_count['Abstract'] == 'CustomerService']\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=tmp['created_at'], y=tmp['tidy_tweet'],\n",
    "#         hoverinfo='x+y',\n",
    "#         mode='lines',\n",
    "#         line=dict(width=0.5,\n",
    "#                   color='rgb(131, 90, 241)'),\n",
    "#         name = 'CustomerService',\n",
    "#         stackgroup='one' # define stack group\n",
    "#     ))\n",
    "#     tmp1 = df_daily_count[df_daily_count['Abstract'] == 'News']\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=tmp1['created_at'], y=tmp1['tidy_tweet'],\n",
    "#         hoverinfo='x+y',\n",
    "#         mode='lines',\n",
    "#         line=dict(width=0.5,\n",
    "#             color='rgb(127, 166, 238)'), \n",
    "#         name = 'News',\n",
    "#         stackgroup='one'\n",
    "#     ))\n",
    "#     tmp2 = df_daily_count[df_daily_count['Abstract'] == 'Price']\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=tmp2['created_at'], y=tmp2['tidy_tweet'],\n",
    "#         hoverinfo='x+y',\n",
    "#         mode='lines',\n",
    "#         line=dict(width=0.5,\n",
    "#             color='rgb(111, 231, 219)'), \n",
    "#         name = 'Price',\n",
    "#         stackgroup='one'\n",
    "#      ))\n",
    "#     tmp3 = df_daily_count[df_daily_count['Abstract'] == 'Products']\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=tmp3['created_at'], y=tmp3['tidy_tweet'],\n",
    "#         hoverinfo='x+y',\n",
    "#         mode='lines',\n",
    "#         line=dict(width=0.5, \n",
    "#             color='rgb(184, 247, 212)'),\n",
    "#         name = 'Products',\n",
    "#         stackgroup='one'\n",
    "#      ))\n",
    "#     tmp4 = df_daily_count[df_daily_count['Abstract'] == 'Recommendation']\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=tmp4['created_at'], y=tmp4['tidy_tweet'],\n",
    "#         hoverinfo='x+y',\n",
    "#         mode='lines',\n",
    "#         line=dict(width=0.5, \n",
    "#             color='rgb(95,158,160)'),\n",
    "#         name = 'Recommendation',\n",
    "#         stackgroup='one'\n",
    "#      ))\n",
    "#     fig.layout.update(title_text='Total activity over time',\n",
    "#                       paper_bgcolor='rgba(0,0,0,0)',\n",
    "#                     plot_bgcolor='rgba(0,0,0,0)',\n",
    "#                      margin=dict(l=50, r=10, t=80, b=80))\n",
    "#     fig.update_layout(legend_orientation=\"h\")\n",
    "\n",
    "    \n",
    "#     plot_url = py.plot(fig, filename='Tweets over time', auto_open=False,)\n",
    "#     #first_plot_url = fig.write_html('first_figure.html', auto_open=True)\n",
    "#     #print(plot_url)\n",
    "#     #fig.show()\n",
    "#     return plot_url\n",
    "#     #return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call twitter activity plot\n",
    "# tweets_over_time_plot_url = tweets_over_time(df_tweets)\n",
    "# tweets_over_time_plot_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method to plot sentiments over all categories\n",
    "# def sentiment_analysis_plot(df_tweets,hashtag):\n",
    "#     #plot sentiments w.r.t abstract\n",
    "#     number_of_tweets = str(len(df_tweets['tidy_tweet']))\n",
    "#     df_sentiment_polarity = df_tweets.groupby(['Abstract','Polarity'],as_index = False).agg({'tidy_tweet': \"count\"})\n",
    "\n",
    "#     tmp = df_sentiment_polarity[df_sentiment_polarity['Polarity'] == 'Positive']\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_trace(go.Bar(    \n",
    "#         y=tmp['Abstract'],\n",
    "#         x=tmp['tidy_tweet'] ,\n",
    "#         name='Positive',\n",
    "#         orientation='h',\n",
    "#         marker=dict(\n",
    "#             color='green',\n",
    "#         )\n",
    "#     ))\n",
    "#     tmp1 = df_sentiment_polarity[df_sentiment_polarity['Polarity'] == 'Neutral']\n",
    "#     fig.add_trace(go.Bar(    \n",
    "#         y=tmp1['Abstract'],\n",
    "#         x=tmp1['tidy_tweet'],\n",
    "#         name='Neutral',\n",
    "#         orientation='h',\n",
    "#         marker=dict(\n",
    "#             color='gold',\n",
    "#         )\n",
    "#     ))\n",
    "#     tmp2 = df_sentiment_polarity[df_sentiment_polarity['Polarity'] == 'Negative']\n",
    "#     fig.add_trace(go.Bar(\n",
    "#         y=tmp2['Abstract'],\n",
    "#         x=tmp2['tidy_tweet'],\n",
    "#         name='Negative',\n",
    "#         orientation='h',\n",
    "#         marker=dict(\n",
    "#             color='red',\n",
    "#         )\n",
    "#     ))\n",
    "#     fig.layout.update(barmode='stack',\n",
    "#         paper_bgcolor='rgba(0,0,0,0)',\n",
    "#         plot_bgcolor='rgba(0,0,0,0)',\n",
    "#         #paper_bgcolor='rgb(248, 248, 255)',\n",
    "#         #plot_bgcolor='rgb(248, 248, 255)',\n",
    "#         margin=dict(l=100, r=10, t=80, b=80),\n",
    "#     )\n",
    "#     fig.layout.update(title_text='Data gathered from the analysis of '+ number_of_tweets +' twitter reviews on '+ hashtag.upper())\n",
    "#     fig.update_layout(legend_orientation=\"h\")\n",
    "\n",
    "#     #fig.show()\n",
    "#     plot_url = py.plot(fig, filename='Tweets Sentiment', auto_open=False,)\n",
    "    \n",
    "#     return plot_url\n",
    "#     #return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call sentiment plot\n",
    "#sentiment_analysis_plot(df_tweets,hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_categorize(df_tweets):\n",
    "#     # categorize sentiment into different categories\n",
    "#     tweets_sent_cat = []\n",
    "#     #df_tweets_1 = {'sentiment': [0.35,0.4]}\n",
    "#     #df_tweets_1 = pd.DataFrame(df_tweets_1)\n",
    "#     for t in df_tweets['sentiment']:\n",
    "#         if t == 0:\n",
    "#             tweets_sent_cat.append(\"fair\") \n",
    "#         elif t <= -0.9:\n",
    "#             tweets_sent_cat.append(\"Unpleasant\")\n",
    "#         elif t > -0.9 and t <= -0.7:\n",
    "#              tweets_sent_cat.append(\"disgust\")\n",
    "#         elif t > -0.7 and t <= -0.4:\n",
    "#              tweets_sent_cat.append(\"upset\")\n",
    "#         elif t < 0 and t > -0.4:\n",
    "#              tweets_sent_cat.append(\"sadness\")\n",
    "#         elif t >= 0.9:\n",
    "#             tweets_sent_cat.append(\"pleasant\")\n",
    "#         elif t >= 0.7 and t < 0.9:\n",
    "#              tweets_sent_cat.append(\"elate\")\n",
    "#         elif t >= 0.4 and t < 0.7:\n",
    "#              tweets_sent_cat.append(\"excited\")\n",
    "#         elif t >0 and t < 0.4:\n",
    "#              tweets_sent_cat.append(\"happy\")\n",
    "\n",
    "#     df_tweets['tweets_sent_cat'] =  tweets_sent_cat\n",
    "    \n",
    "#     return df_tweets\n",
    "\n",
    "# # call sentiment  categories\n",
    "# #df_tweets = sentiment_categorize(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# # method to plot spiral plot of sentiments\n",
    "# def tweets_spiral(df_tweets):\n",
    "#     df_tweets_sent_cat = df_tweets.groupby(['Abstract','tweets_sent_cat'],as_index = False).agg({'tidy_tweet': \"count\"})\n",
    "#     #df_tweets_sent_cat\n",
    "\n",
    "#     #plot the chart\n",
    "#     fig = px.bar_polar(df_tweets_sent_cat, r=\"tidy_tweet\", theta=\"tweets_sent_cat\",\n",
    "#                         color=\"Abstract\",template=\"seaborn\",\n",
    "#                        color_discrete_sequence= px.colors.sequential.Plasma[-2::-1])\n",
    "#     fig.layout.update(title_text='Sentiments spiral')\n",
    "#     fig.update_layout(legend_orientation=\"h\",\n",
    "#                       margin=dict(l=80, r=80, t=80, b=100),\n",
    "#                      paper_bgcolor='rgba(0,0,0,0)')\n",
    "    \n",
    "#     #fig.show()\n",
    "#     plot_url = py.plot(fig, filename='Tweets Spiral Sentiment', auto_open=False,)\n",
    "    \n",
    "#     return plot_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # call sentiment spliral plot\n",
    "# tweets_spiral(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method to display all tweets for the given hashtag in a table\n",
    "# def tweets_table(df_tweets,hashtag):\n",
    "#     df_tweets = df_tweets.loc[1:3000]\n",
    "#     values = [list(df_tweets['created_at']), #1st col\n",
    "#       list(df_tweets['tidy_tweet']),\n",
    "#       list(df_tweets['Abstract'])]\n",
    "\n",
    "\n",
    "#     fig = go.Figure(data=[go.Table(\n",
    "#       columnorder = [1,2,3],\n",
    "#       columnwidth = [80,400,80],\n",
    "#       header = dict(\n",
    "#         values = [['<b>CREATED_AT</b>'],\n",
    "#                     ['<b>TWEETS</b>'],\n",
    "#                  ['<b>ABSTRACT</b>']],\n",
    "#         line_color='darkslategray',\n",
    "#         fill_color='royalblue',\n",
    "#         align=['left','center'],\n",
    "#         font=dict(color='white', size=12),\n",
    "#         height=40\n",
    "#       ),\n",
    "#       cells=dict(\n",
    "#         values=values,\n",
    "#         line_color='darkslategray',\n",
    "#         fill=dict(color=['white','white', 'paleturquoise']),\n",
    "#         align=['left', 'center'],\n",
    "#         font_size=12,\n",
    "#         height=30)\n",
    "#         )\n",
    "#     ])\n",
    "#     fig.layout.update(title_text='Tweets on #'+hashtag.upper(),\n",
    "#                       paper_bgcolor='rgba(0,0,0,0)',\n",
    "#                      font=dict(\n",
    "#                              family=\"Courier New, monospace\",\n",
    "#                              size=18,\n",
    "#                              color=\"#7f7f7f\"\n",
    "#             ))\n",
    "\n",
    "    \n",
    "#     #fig.show()\n",
    "#     plot_url = py.plot(fig, filename='Tweets Table', auto_open=False,weight = \"bold\")\n",
    "    \n",
    "#     return plot_url\n",
    "#     #return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets table\n",
    "#tweets_table(df_tweets,hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_html_string(hashtag,p1,p2,t1,p3):\n",
    "#     print(hashtag)\n",
    "#     html_string = '''\n",
    "#     <html>\n",
    "#         <head>\n",
    "#             <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\">\n",
    "#             <style>body{ margin:0 100; background:whitesmoke; }</style>\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1><img src = \"https://1000logos.net/wp-content/uploads/2017/06/Twitter-Logo.png\" alt = \"Twitter\" \n",
    "#             width=\"100\" height=\"100\">Twitter Review Analysis on #'''+ hashtag.upper() + ''' </h1> \n",
    "#             <!-- Add icon library -->\n",
    "        \n",
    "            \n",
    "#             <!-- *** Section 1 *** ---> \n",
    "#             <!--<h2>Tweets over time</h2>-->\n",
    "#             <iframe width=\"700\" height=\"400\" frameborder=\"0\" seamless=\"seamless\" scrolling=\"no\" \\\n",
    "#     src=\"''' + p1 + '''\"></iframe>\n",
    "#              <!-- *** Section 2 *** --->\n",
    "#             <!--<h2>Tweets Sentiment Analysis</h2>-->\n",
    "#             <iframe width= \"600\" height=\"400\" frameborder=\"0\" seamless=\"seamless\" scrolling=\"no\" \\\n",
    "#     src=\"''' + p2 + '''\"></iframe> \n",
    "#             <!--<h2>Twitter Reviews</h2>-->\n",
    "#             <iframe width=\"800\" height=\"500\" frameborder=\"0\" seamless=\"seamless\" scrolling=\"no\" \\\n",
    "#     src=\"''' + t1 + '''\"></iframe> \n",
    "#             <!--<h2>Sentiment Spiral</h2>-->\n",
    "#             <iframe width=\"500\" height=\"500\" frameborder=\"0\" seamless=\"seamless\" scrolling=\"no\" \\\n",
    "#     src=\"''' + p3 + '''\"></iframe> \n",
    "\n",
    "#         </body>\n",
    "#     </html>'''\n",
    "    \n",
    "#     return html_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method to create a report html\n",
    "# def write_to_html(hashtag,html_string):\n",
    "#     f = open('C:/Users/Musarath/Musarath/Anly699_Project/Templates/'+hashtag+'_report.html','w')\n",
    "#     f.write(html_string)\n",
    "#     f.close()\n",
    "    \n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unblock this code to test locally ##\n",
    "\n",
    "# import chart_studio\n",
    "# import plotly.express as px\n",
    "# #chart_studio.tools.set_credentials_file(username='musarath', api_key='V2wKW2XdT3cggRuR3916')\n",
    "# #import plotly.plotly as py\n",
    "# hashtag = \"mus\"\n",
    "# # plot generations\n",
    "# df_tweets = sentiment_categorize(df_tweets)\n",
    "\n",
    "# tweets_over_time_plot_url = tweets_over_time(df_tweets)\n",
    "# #print(tweets_over_time_plot_url)\n",
    "# tweets_sentiment_plot_url = sentiment_analysis_plot(df_tweets,hashtag)\n",
    "# #print(tweets_sentiment_plot_url)\n",
    "# all_tweets_table = tweets_table(df_tweets,hashtag)\n",
    "# #print(all_tweets_table)\n",
    "# sentiment_spiral_plot_url = tweets_spiral(df_tweets)\n",
    "# #print(sentiment_spiral_plot_url)\n",
    "# html_string = define_html_string(tweets_over_time_plot_url,tweets_sentiment_plot_url,all_tweets_table,sentiment_spiral_plot_url)\n",
    "# write_to_html(hashtag,html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of positive, negative and neutral tweets\n",
    "#np.unique(df_tweets['Polarity'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract sentiment\n",
    "# df_sentiments = pd.DataFrame(df_tweets.groupby('Abstract')['sentiment'].mean(),columns = ['sentiment'])\n",
    "# df_sentiments.reset_index() \n",
    "# df_sentiments['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #plot sentiments w.r.t abstract\n",
    "# number_of_tweets = str(len(df_tweets['tidy_tweet']))\n",
    "# df_sentiment_polarity = df_tweets.groupby(['Abstract','Polarity'],as_index = False).agg({'tidy_tweet': \"count\"})\n",
    "\n",
    "# fig = px.bar(df_sentiment_polarity, x=\"Abstract\", y=\"tidy_tweet\", color=[\"green\",\"gold\",\"red\"])\n",
    "# fig.update_layout(title_text='Data gathered from the analysis of '+ number_of_tweets +' twitter reviews on '+ hashtag)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tweets scatter plot\n",
    "# fig = go.Figure(data=go.Scatter(\n",
    "#     y = df_tweets['sentiment'],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=16,\n",
    "#         color= df_tweets['sentiment'], #set color equal to a variable\n",
    "#         colorscale='Viridis', # one of plotly colorscales\n",
    "#         showscale=True\n",
    "#     )\n",
    "# ))\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global str\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "# # to get information from my-form page\n",
    "# @app.route('/')\n",
    "# def my_form():\n",
    "#     return render_template('my-report.html')\n",
    "\n",
    "# # to post the result to web page\n",
    "# @app.route('/',methods=['POST'])\n",
    "# def my_hashtag_report():\n",
    "#     # read the batch file\n",
    "#     hashtag = str(request.form['string'])   # get the url and store in varaible\n",
    "#     print(hashtag)\n",
    "\n",
    "#     # get data\n",
    "#     tweets_data = twitter_data_download(hashtag)\n",
    "#     # tag the tweets\n",
    "#     df_tweets = twitter_abstract(tweets_data,xtrain_tfidf_word,train_y)\n",
    "\n",
    "#     # sentiment\n",
    "#     df_tweets = tweets_sentiment(df_tweets)\n",
    "#     # polarity\n",
    "#     df_tweets = tweets_polarity(df_tweets)\n",
    "#     # sentiment categorize\n",
    "#     df_tweets = sentiment_categorize(df_tweets)\n",
    "\n",
    "#     # plot generations\n",
    "#     tweets_over_time_plot_url = tweets_over_time(df_tweets)\n",
    "#     tweets_sentiment_plot_url = sentiment_analysis_plot(df_tweets,hashtag)\n",
    "#     all_tweets_table = tweets_table(df_tweets,hashtag)\n",
    "#     sentiment_spiral_plot_url = tweets_spiral(df_tweets)\n",
    "\n",
    "#     # create report\n",
    "#     html_string = define_html_string(hashtag,tweets_over_time_plot_url,tweets_sentiment_plot_url,all_tweets_table,sentiment_spiral_plot_url)\n",
    "#     write_to_html(hashtag,html_string)\n",
    "\n",
    "#     # open the report\n",
    "#     #webbrowser.open_new_tab( 'C:/Users/Musarath/Musarath/Anly699_Project/'+hashtag+'_report.html')    \n",
    "\n",
    "#     filename = str(hashtag+'_report.html')\n",
    "#     return render_template(filename)\n",
    "        \n",
    "        \n",
    "# # main function\n",
    "# if __name__ == '__main__':\n",
    "#     #app.run(host='localhost',port=5051)\n",
    "#     app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # positive word cloud\n",
    "# normal_words =' '.join([text for text in df_tweets['tidy_tweet'][df_tweets['sentiment'] > 0]])\n",
    "\n",
    "# wordcloud = WordCloud(width=800, height=500, random_state=123, max_font_size=110,background_color= 'white',max_words=50).generate(normal_words)\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # negative word cloud\n",
    "# negative_words = ' '.join([text for text in df_tweets['tidy_tweet'][df_tweets['sentiment'] < 0]])\n",
    "# wordcloud = WordCloud(width=800, height=500,\n",
    "# random_state=21, max_font_size=110,background_color= 'white',max_words=50).generate(negative_words)\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
